// æ£€æµ‹æ˜¯å¦åœ¨ Vercel ç¯å¢ƒä¸­è¿è¡Œ
const isVercelEnvironment = process.env.VERCEL === '1' || process.env.VERCEL_ENV !== undefined

// è¶…æ—¶é…ç½®å¸¸é‡ - åŸºäº Vercel å®˜æ–¹é™åˆ¶ä¼˜åŒ–
const TIMEOUT_CONFIG = {
  CONNECTION_TEST: isVercelEnvironment ? 10000 : 15000,    // Vercel: 10ç§’, å…¶ä»–: 15ç§’
  SIMPLE_CHAT: isVercelEnvironment ? 45000 : 55000,        // Vercel: 45ç§’, å…¶ä»–: 55ç§’
  COMPLEX_ANALYSIS: isVercelEnvironment ? 50000 : 90000,   // Vercel: 50ç§’, å…¶ä»–: 90ç§’
  STREAM_RESPONSE: isVercelEnvironment ? 55000 : 120000,   // Vercel: 55ç§’, å…¶ä»–: 120ç§’
  IMAGE_PROCESSING: isVercelEnvironment ? 40000 : 75000,   // Vercel: 40ç§’, å…¶ä»–: 75ç§’
  DEFAULT: isVercelEnvironment ? 45000 : 55000,            // Vercel: 45ç§’, å…¶ä»–: 55ç§’
  SMART_SUGGESTIONS: isVercelEnvironment ? 50000 : 55000   // Vercel: 50ç§’, å…¶ä»–: 55ç§’
} as const

// é€šç”¨çš„ OpenAI å…¼å®¹å®¢æˆ·ç«¯
export class OpenAICompatibleClient {
  private baseUrl: string
  private apiKey: string

  constructor(baseUrl: string, apiKey: string) {
    // ç¡®ä¿baseUrlæ ¼å¼æ­£ç¡®
    this.baseUrl = baseUrl.endsWith("/") ? baseUrl.slice(0, -1) : baseUrl
    // å¦‚æœbaseUrlå·²ç»åŒ…å«/v1ï¼Œä¸è¦é‡å¤æ·»åŠ 
    if (this.baseUrl.endsWith("/v1")) {
      this.baseUrl = this.baseUrl.slice(0, -3)
    }
    this.apiKey = apiKey

    //console.log("OpenAI Client initialized:", {
    //  baseUrl: this.baseUrl,
    //  hasApiKey: !!this.apiKey,
    //})
  }

  async createChatCompletion(params: {
    model: string
    messages: Array<{ role: string; content: string | Array<any> }>
    response_format?: { type: string }
    stream?: boolean
    max_tokens?: number
  }) {
    const url = `${this.baseUrl}/v1/chat/completions`
    //console.log("Making request to:", url)
    //console.log("Request params:", {
    //  model: params.model,
    //  messageCount: params.messages.length,
    //  stream: params.stream,
    //  hasResponseFormat: !!params.response_format,
    //})

    const requestBody = {
      model: params.model,
      messages: params.messages,
      stream: params.stream || false,
      ...(params.response_format && { response_format: params.response_format }),
      ...(params.max_tokens && { max_tokens: params.max_tokens }),
    }

    // ğŸ› è°ƒè¯•æ—¥å¿— - é¿å…æ‰“å°å®Œæ•´çš„ base64 å›¾ç‰‡æ•°æ®ï¼ˆç”Ÿäº§ç¯å¢ƒå·²ç¦ç”¨ï¼‰
    // const debugRequestBody = {
    //   ...requestBody,
    //   messages: requestBody.messages.map((msg: any) => {
    //     if (msg.content && Array.isArray(msg.content)) {
    //       return {
    //         ...msg,
    //         content: msg.content.map((item: any) => {
    //           if (item.type === 'image_url' && item.image_url?.url) {
    //             const url = item.image_url.url
    //             const preview = url.length > 100 ? `${url.substring(0, 50)}...[${url.length} chars total]` : url
    //             return {
    //               ...item,
    //               image_url: {
    //                 ...item.image_url,
    //                 url: preview
    //               }
    //             }
    //           }
    //           return item
    //         })
    //       }
    //     }
    //     return msg
    //   })
    // }
    //console.log("Request body (base64 truncated):", JSON.stringify(debugRequestBody, null, 2))

    try {
      // åˆ›å»º AbortController ç”¨äºè¶…æ—¶æ§åˆ¶
      const controller = new AbortController()

      // æ ¹æ®è¯·æ±‚ç±»å‹é€‰æ‹©åˆé€‚çš„è¶…æ—¶æ—¶é—´
      let timeout: number = TIMEOUT_CONFIG.DEFAULT
      if (params.stream) {
        timeout = TIMEOUT_CONFIG.STREAM_RESPONSE
      } else if (requestBody.messages.some((msg: any) =>
        Array.isArray(msg.content) && msg.content.some((item: any) => item.type === 'image_url')
      )) {
        timeout = TIMEOUT_CONFIG.IMAGE_PROCESSING
      } else if (params.response_format?.type === 'json_object') {
        // JSON æ ¼å¼å“åº”é€šå¸¸ç”¨äºæ™ºèƒ½å»ºè®®ç­‰å¤æ‚åˆ†æ
        timeout = TIMEOUT_CONFIG.SMART_SUGGESTIONS
      }

      const timeoutId = setTimeout(() => controller.abort(), timeout)

      const response = await fetch(url, {
        method: "POST",
        headers: {
          "Content-Type": "application/json; charset=utf-8",
          Authorization: `Bearer ${this.apiKey}`,
          "Accept": "application/json; charset=utf-8",
        },
        body: JSON.stringify(requestBody),
        signal: controller.signal,
      })

      clearTimeout(timeoutId)

      console.log("Response status:", response.status)
      //console.log("Response headers:", Object.fromEntries(response.headers.entries()))

      if (!response.ok) {
        const errorText = await response.text()
        console.error("API Error Response:", errorText)
        throw new Error(`API request failed: ${response.status} ${response.statusText} - ${errorText}`)
      }

      return response
    } catch (error) {
      console.error("Fetch error:", error)

      // æä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
      if (error instanceof Error) {
        if (error.name === 'AbortError') {
          throw new Error(`è¯·æ±‚è¶…æ—¶ï¼šè¿æ¥åˆ° ${this.baseUrl} æœªåœ¨é¢„æœŸæ—¶é—´å†…å“åº”ã€‚è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–APIæœåŠ¡çŠ¶æ€ã€‚`)
        } else if (error.message.includes('ENOTFOUND') || error.message.includes('ECONNREFUSED')) {
          throw new Error(`ç½‘ç»œè¿æ¥å¤±è´¥ï¼šæ— æ³•è¿æ¥åˆ° ${this.baseUrl}ã€‚è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒAPIåœ°å€æ˜¯å¦æ­£ç¡®ã€‚`)
        } else if (error.message.includes('CERT') || error.message.includes('certificate')) {
          throw new Error(`SSLè¯ä¹¦é”™è¯¯ï¼šè¿æ¥åˆ° ${this.baseUrl} æ—¶é‡åˆ°è¯ä¹¦é—®é¢˜ã€‚`)
        }
      }

      throw new Error(`APIè¯·æ±‚å¤±è´¥ï¼š${error instanceof Error ? error.message : String(error)}`)
    }
  }

  async generateText(params: {
    model: string
    prompt: string
    images?: string[]
    response_format?: { type: string }
    max_tokens?: number
  }) {
    //console.log("Generating text with params:", {
    //  model: params.model,
    //  promptLength: params.prompt.length,
    //  imageCount: params.images?.length || 0,
    //  hasResponseFormat: !!params.response_format,
    //})

    const messages: Array<{ role: string; content: string | Array<any> }> = []

    if (params.images && params.images.length > 0) {
      // è§†è§‰æ¨¡å‹è¯·æ±‚
      const content = [
        { type: "text", text: params.prompt },
        ...params.images.map((image) => ({
          type: "image_url",
          image_url: { url: image },
        })),
      ]
      messages.push({ role: "user", content })
    } else {
      // æ™®é€šæ–‡æœ¬è¯·æ±‚
      messages.push({ role: "user", content: params.prompt })
    }

    const response = await this.createChatCompletion({
      model: params.model,
      messages,
      response_format: params.response_format,
      max_tokens: params.max_tokens,
    })

    const result = await response.json()
    //console.log("Generate text result:", {
    //  hasChoices: !!result.choices,
    //  choiceCount: result.choices?.length || 0,
    //  firstChoiceContent: result.choices?.[0]?.message?.content?.substring(0, 100) + "...",
    //})

    return {
      text: result.choices[0]?.message?.content || "",
    }
  }

  async streamText(params: {
    model: string
    messages: Array<{ role: string; content: string; images?: string[] }>
    system?: string
  }) {
    console.log("Streaming text with params:", {
      model: params.model,
      messageCount: params.messages.length,
      hasSystem: !!params.system,
      hasImages: params.messages.some(msg => msg.images && msg.images.length > 0)
    })

    // è½¬æ¢æ¶ˆæ¯æ ¼å¼ä»¥æ”¯æŒå›¾ç‰‡
    const messages: Array<{ role: string; content: string | Array<any> }> = params.messages.map(msg => {
      if (msg.images && msg.images.length > 0) {
        // åŒ…å«å›¾ç‰‡çš„æ¶ˆæ¯
        const content = [
          { type: "text", text: msg.content },
          ...msg.images.map((image) => ({
            type: "image_url",
            image_url: { url: image },
          })),
        ]
        return { role: msg.role, content }
      } else {
        // çº¯æ–‡æœ¬æ¶ˆæ¯
        return { role: msg.role, content: msg.content }
      }
    })

    if (params.system) {
      messages.unshift({ role: "system", content: params.system })
    }

    const response = await this.createChatCompletion({
      model: params.model,
      messages,
      stream: true,
    })

    return response
  }

  // è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
  async listModels() {
    const url = `${this.baseUrl}/v1/models`
    console.log("Listing models from:", url)

    try {
      // åˆ›å»º AbortController ç”¨äºè¶…æ—¶æ§åˆ¶
      const controller = new AbortController()
      const timeoutId = setTimeout(() => controller.abort(), TIMEOUT_CONFIG.CONNECTION_TEST)

      const response = await fetch(url, {
        method: "GET",
        headers: {
          Authorization: `Bearer ${this.apiKey}`,
        },
        signal: controller.signal,
      })

      clearTimeout(timeoutId)

      console.log("List models response status:", response.status)

      if (!response.ok) {
        const errorText = await response.text()
        console.error("List models error:", errorText)
        throw new Error(`Failed to fetch models: ${response.status} ${response.statusText} - ${errorText}`)
      }

      const result = await response.json()
      console.log("Models fetched:", result.data?.length || 0)
      return result
    } catch (error) {
      console.error("List models fetch error:", error)

      // æä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
      if (error instanceof Error) {
        if (error.name === 'AbortError') {
          throw new Error(`è·å–æ¨¡å‹åˆ—è¡¨è¶…æ—¶ï¼šè¿æ¥åˆ° ${this.baseUrl} è¶…è¿‡${TIMEOUT_CONFIG.CONNECTION_TEST/1000}ç§’æœªå“åº”ã€‚`)
        } else if (error.message.includes('ENOTFOUND') || error.message.includes('ECONNREFUSED')) {
          throw new Error(`ç½‘ç»œè¿æ¥å¤±è´¥ï¼šæ— æ³•è¿æ¥åˆ° ${this.baseUrl}ã€‚è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒAPIåœ°å€ã€‚`)
        }
      }

      throw new Error(`è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥ï¼š${error instanceof Error ? error.message : String(error)}`)
    }
  }
}

// æ¨¡å‹ç±»å‹æ¥å£
export interface OpenAIModel {
  id: string
  object: string
  created: number
  owned_by: string
}

export interface OpenAIModelList {
  object: string
  data: OpenAIModel[]
}
